{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223afdc3",
   "metadata": {},
   "source": [
    "# Finnt- Feature Engineering & Data Preprocessing\n",
    "\n",
    "**Advanced Feature Engineering for Personalized Loan Offer Engine**\n",
    "\n",
    "This notebook transforms the raw Lending Club dataset into a clean, feature-rich dataset ready for machine learning. We'll engineer our dual target variables and create robust features for our optimization models.\n",
    "\n",
    "## Objectives:\n",
    "1. **Target Engineering**: Create `is_default` for P(Default) model\n",
    "2. **Data Cleaning**: Handle missing values and data quality issues\n",
    "3. **Feature Transformation**: Convert categorical and date features\n",
    "4. **Data Type Optimization**: Ensure proper data types for modeling\n",
    "5. **MLOps Integration**: Version the processed dataset with DVC\n",
    "\n",
    "**Date**: July 8, 2025  \n",
    "**Phase**: 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d641c5",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import essential libraries for data manipulation, feature engineering, and date processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986b979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Visualization for data exploration\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262f0e4",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data\n",
    "\n",
    "Load the original Lending Club dataset from our DVC-tracked data directory for comprehensive feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b686ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw dataset from: ../data/raw/loan.csv\n",
      "Raw dataset loaded successfully!\n",
      "Original dataset shape: 2,260,668 rows × 145 columns\n",
      "Raw dataset loaded successfully!\n",
      "Original dataset shape: 2,260,668 rows × 145 columns\n",
      "Memory usage: 5942.3 MB\n",
      "Memory usage: 5942.3 MB\n",
      "Working copy created for feature engineering\n",
      "Working copy created for feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Load the raw dataset\n",
    "data_path = '../data/raw/loan.csv'\n",
    "print(f\"Loading raw dataset from: {data_path}\")\n",
    "\n",
    "# Read CSV with optimized settings\n",
    "df_raw = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "print(f\"Raw dataset loaded successfully!\")\n",
    "print(f\"Original dataset shape: {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Create a working copy for feature engineering\n",
    "df = df_raw.copy()\n",
    "print(f\"Working copy created for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea6cdb",
   "metadata": {},
   "source": [
    "## 3. Target Variable Engineering\n",
    "\n",
    "**Critical Step**: Create our target variables for the dual-model approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac841b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAN STATUS ANALYSIS\n",
      "==================================================\n",
      "Current loan statuses in dataset:\n",
      "  Fully Paid: 1,041,952 (46.09%)\n",
      "  Current: 919,695 (40.68%)\n",
      "  Charged Off: 261,655 (11.57%)\n",
      "  Late (31-120 days): 21,897 (0.97%)\n",
      "  In Grace Period: 8,952 (0.40%)\n",
      "  Late (16-30 days): 3,737 (0.17%)\n",
      "  Does not meet the credit policy. Status:Fully Paid: 1,988 (0.09%)\n",
      "  Does not meet the credit policy. Status:Charged Off: 761 (0.03%)\n",
      "  Default: 31 (0.00%)\n",
      "\n",
      "Total unique statuses: 9\n",
      "\n",
      "DEFINING DEFAULT EVENTS:\n",
      "The following loan statuses will be classified as defaults:\n",
      "  Charged Off: 261,655 (11.57%)\n",
      "  Default: 31 (0.00%)\n",
      "  Does not meet the credit policy. Status:Charged Off: 761 (0.03%)\n",
      "  Late (31-120 days): 21,897 (0.97%)\n"
     ]
    }
   ],
   "source": [
    "# First, examine the unique loan statuses in our dataset\n",
    "print(\"LOAN STATUS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "loan_status_counts = df['loan_status'].value_counts()\n",
    "print(\"Current loan statuses in dataset:\")\n",
    "for status, count in loan_status_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {status}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal unique statuses: {df['loan_status'].nunique()}\")\n",
    "\n",
    "# Define default events for our risk model\n",
    "default_statuses = [\n",
    "    'Charged Off',\n",
    "    'Default', \n",
    "    'Does not meet the credit policy. Status:Charged Off',\n",
    "    'Late (31-120 days)'\n",
    "]\n",
    "\n",
    "print(f\"\\nDEFINING DEFAULT EVENTS:\")\n",
    "print(\"The following loan statuses will be classified as defaults:\")\n",
    "for status in default_statuses:\n",
    "    if status in loan_status_counts:\n",
    "        count = loan_status_counts[status]\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {status}: {count:,} ({pct:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {status}: Not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad68305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TARGET VARIABLE: is_default\n",
      "==================================================\n",
      "Overall Default Rate: 12.58%\n",
      "Non-Default Rate: 87.42%\n",
      "\n",
      "Class Distribution:\n",
      "  0 (Non-Default): 1,976,324 (87.42%)\n",
      "  1 (Default): 284,344 (12.58%)\n",
      "\n",
      "Class Balance Analysis:\n",
      "Non-Default to Default Ratio: 7.0:1\n",
      "Moderate class imbalance - consider balancing techniques\n"
     ]
    }
   ],
   "source": [
    "# Create the binary default target variable\n",
    "df['is_default'] = df['loan_status'].isin(default_statuses).astype(int)\n",
    "\n",
    "# Analyze the target variable distribution\n",
    "default_rate = df['is_default'].mean() * 100\n",
    "print(f\"\\nTARGET VARIABLE: is_default\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Default Rate: {default_rate:.2f}%\")\n",
    "print(f\"Non-Default Rate: {100-default_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "target_distribution = df['is_default'].value_counts().sort_index()\n",
    "for value, count in target_distribution.items():\n",
    "    label = \"Default\" if value == 1 else \"Non-Default\"\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {value} ({label}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "class_ratio = target_distribution[0] / target_distribution[1]\n",
    "print(f\"\\nClass Balance Analysis:\")\n",
    "print(f\"Non-Default to Default Ratio: {class_ratio:.1f}:1\")\n",
    "if class_ratio > 10:\n",
    "    print(\"Significant class imbalance detected - will need balancing techniques\")\n",
    "elif class_ratio > 5:\n",
    "    print(\"Moderate class imbalance - consider balancing techniques\")\n",
    "else:\n",
    "    print(\"Reasonable class balance for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef9e06",
   "metadata": {},
   "source": [
    "### P(Acceptance) Target Strategy\n",
    "\n",
    "**Important Note**: All loans in this historical dataset were **implicitly accepted** by customers (since they exist in the dataset). Therefore, creating a simple binary acceptance target would result in all `1`s, which isn't useful for modeling.\n",
    "\n",
    "**Our Strategy for P(Acceptance) Modeling**:\n",
    "1. **Phase 3**: We'll use interest rate sensitivity analysis to create synthetic acceptance scenarios\n",
    "2. **Feature Engineering**: Derive proxy variables like `requested_vs_funded_ratio` to understand acceptance patterns\n",
    "3. **External Data**: In production, we'd collect customer response data to loan offers at different rates\n",
    "4. **Simulation**: Create acceptance probability curves based on rate spreads and customer segments\n",
    "\n",
    "For now, we focus on the **P(Default) model** which has a clear, well-defined target variable (`is_default`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98f200",
   "metadata": {},
   "source": [
    "## 4. Initial Feature Selection & Data Leakage Prevention\n",
    "\n",
    "**Critical MLOps Step**: Remove features that would not be available at loan application time or have excessive missing values. This prevents data leakage and ensures our model is realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1d9196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUE ANALYSIS\n",
      "==================================================\n",
      "Columns with >40% missing data (46 total):\n",
      "  id: 100.0% missing\n",
      "  url: 100.0% missing\n",
      "  member_id: 100.0% missing\n",
      "  orig_projected_additional_accrued_interest: 99.6% missing\n",
      "  hardship_length: 99.5% missing\n",
      "  hardship_reason: 99.5% missing\n",
      "  hardship_status: 99.5% missing\n",
      "  deferral_term: 99.5% missing\n",
      "  hardship_amount: 99.5% missing\n",
      "  hardship_start_date: 99.5% missing\n",
      "  hardship_end_date: 99.5% missing\n",
      "  payment_plan_start_date: 99.5% missing\n",
      "  hardship_loan_status: 99.5% missing\n",
      "  hardship_dpd: 99.5% missing\n",
      "  hardship_payoff_balance_amount: 99.5% missing\n",
      "  hardship_last_payment_amount: 99.5% missing\n",
      "  hardship_type: 99.5% missing\n",
      "  debt_settlement_flag_date: 98.5% missing\n",
      "  settlement_status: 98.5% missing\n",
      "  settlement_date: 98.5% missing\n",
      "  settlement_amount: 98.5% missing\n",
      "  settlement_percentage: 98.5% missing\n",
      "  settlement_term: 98.5% missing\n",
      "  sec_app_mths_since_last_major_derog: 98.4% missing\n",
      "  sec_app_revol_util: 95.3% missing\n",
      "  revol_bal_joint: 95.2% missing\n",
      "  sec_app_num_rev_accts: 95.2% missing\n",
      "  sec_app_inq_last_6mths: 95.2% missing\n",
      "  sec_app_collections_12_mths_ex_med: 95.2% missing\n",
      "  sec_app_earliest_cr_line: 95.2% missing\n",
      "  sec_app_chargeoff_within_12_mths: 95.2% missing\n",
      "  sec_app_mort_acc: 95.2% missing\n",
      "  sec_app_open_acc: 95.2% missing\n",
      "  sec_app_open_act_il: 95.2% missing\n",
      "  verification_status_joint: 94.9% missing\n",
      "  dti_joint: 94.7% missing\n",
      "  annual_inc_joint: 94.7% missing\n",
      "  desc: 94.4% missing\n",
      "  mths_since_last_record: 84.1% missing\n",
      "  mths_since_recent_bc_dlq: 77.0% missing\n",
      "  mths_since_last_major_derog: 74.3% missing\n",
      "  mths_since_recent_revol_delinq: 67.3% missing\n",
      "  next_pymnt_d: 57.7% missing\n",
      "  mths_since_last_delinq: 51.2% missing\n",
      "  il_util: 47.3% missing\n",
      "  mths_since_rcnt_il: 40.3% missing\n",
      "\n",
      "DATA LEAKAGE PREVENTION\n",
      "==================================================\n",
      "Columns to drop (would not be available at loan application time):\n",
      "  id\n",
      "  member_id\n",
      "  url\n",
      "  zip_code\n",
      "  loan_status\n",
      "  total_pymnt\n",
      "  total_pymnt_inv\n",
      "  total_rec_prncp\n",
      "  total_rec_int\n",
      "  total_rec_late_fee\n",
      "  recoveries\n",
      "  collection_recovery_fee\n",
      "  last_pymnt_d\n",
      "  last_pymnt_amnt\n",
      "  next_pymnt_d\n",
      "  last_credit_pull_d\n",
      "  policy_code\n",
      "  application_type\n",
      "  hardship_flag\n",
      "  disbursement_method\n",
      "  debt_settlement_flag\n",
      "\n",
      "SUMMARY\n",
      "==============================\n",
      "Original columns: 146\n",
      "Columns to drop: 63\n",
      "Remaining columns: 83\n",
      "is_default protected: True\n",
      "Columns with >40% missing data (46 total):\n",
      "  id: 100.0% missing\n",
      "  url: 100.0% missing\n",
      "  member_id: 100.0% missing\n",
      "  orig_projected_additional_accrued_interest: 99.6% missing\n",
      "  hardship_length: 99.5% missing\n",
      "  hardship_reason: 99.5% missing\n",
      "  hardship_status: 99.5% missing\n",
      "  deferral_term: 99.5% missing\n",
      "  hardship_amount: 99.5% missing\n",
      "  hardship_start_date: 99.5% missing\n",
      "  hardship_end_date: 99.5% missing\n",
      "  payment_plan_start_date: 99.5% missing\n",
      "  hardship_loan_status: 99.5% missing\n",
      "  hardship_dpd: 99.5% missing\n",
      "  hardship_payoff_balance_amount: 99.5% missing\n",
      "  hardship_last_payment_amount: 99.5% missing\n",
      "  hardship_type: 99.5% missing\n",
      "  debt_settlement_flag_date: 98.5% missing\n",
      "  settlement_status: 98.5% missing\n",
      "  settlement_date: 98.5% missing\n",
      "  settlement_amount: 98.5% missing\n",
      "  settlement_percentage: 98.5% missing\n",
      "  settlement_term: 98.5% missing\n",
      "  sec_app_mths_since_last_major_derog: 98.4% missing\n",
      "  sec_app_revol_util: 95.3% missing\n",
      "  revol_bal_joint: 95.2% missing\n",
      "  sec_app_num_rev_accts: 95.2% missing\n",
      "  sec_app_inq_last_6mths: 95.2% missing\n",
      "  sec_app_collections_12_mths_ex_med: 95.2% missing\n",
      "  sec_app_earliest_cr_line: 95.2% missing\n",
      "  sec_app_chargeoff_within_12_mths: 95.2% missing\n",
      "  sec_app_mort_acc: 95.2% missing\n",
      "  sec_app_open_acc: 95.2% missing\n",
      "  sec_app_open_act_il: 95.2% missing\n",
      "  verification_status_joint: 94.9% missing\n",
      "  dti_joint: 94.7% missing\n",
      "  annual_inc_joint: 94.7% missing\n",
      "  desc: 94.4% missing\n",
      "  mths_since_last_record: 84.1% missing\n",
      "  mths_since_recent_bc_dlq: 77.0% missing\n",
      "  mths_since_last_major_derog: 74.3% missing\n",
      "  mths_since_recent_revol_delinq: 67.3% missing\n",
      "  next_pymnt_d: 57.7% missing\n",
      "  mths_since_last_delinq: 51.2% missing\n",
      "  il_util: 47.3% missing\n",
      "  mths_since_rcnt_il: 40.3% missing\n",
      "\n",
      "DATA LEAKAGE PREVENTION\n",
      "==================================================\n",
      "Columns to drop (would not be available at loan application time):\n",
      "  id\n",
      "  member_id\n",
      "  url\n",
      "  zip_code\n",
      "  loan_status\n",
      "  total_pymnt\n",
      "  total_pymnt_inv\n",
      "  total_rec_prncp\n",
      "  total_rec_int\n",
      "  total_rec_late_fee\n",
      "  recoveries\n",
      "  collection_recovery_fee\n",
      "  last_pymnt_d\n",
      "  last_pymnt_amnt\n",
      "  next_pymnt_d\n",
      "  last_credit_pull_d\n",
      "  policy_code\n",
      "  application_type\n",
      "  hardship_flag\n",
      "  disbursement_method\n",
      "  debt_settlement_flag\n",
      "\n",
      "SUMMARY\n",
      "==============================\n",
      "Original columns: 146\n",
      "Columns to drop: 63\n",
      "Remaining columns: 83\n",
      "is_default protected: True\n"
     ]
    }
   ],
   "source": [
    "# First, identify columns with high missing values (>40%)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "high_missing_cols = missing_pct[missing_pct > 40].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Columns with >40% missing data ({len(high_missing_cols)} total):\")\n",
    "for col, pct in high_missing_cols.items():\n",
    "    print(f\"  {col}: {pct:.1f}% missing\")\n",
    "\n",
    "# Define columns that cause data leakage (not available at application time)\n",
    "leakage_columns = [\n",
    "    # Loan identifiers (not predictive)\n",
    "    'id', 'member_id', 'url',\n",
    "    \n",
    "    # Geographic data (too granular, privacy concerns)\n",
    "    'zip_code',\n",
    "    \n",
    "    # Target variable (we have our engineered version)\n",
    "    'loan_status',\n",
    "    \n",
    "    # Payment history (not available at application time)\n",
    "    'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
    "    'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "    'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
    "    \n",
    "    # Policy codes and internal flags\n",
    "    'policy_code', 'application_type',\n",
    "    \n",
    "    # Post-loan information\n",
    "    'hardship_flag', 'disbursement_method', 'debt_settlement_flag'\n",
    "]\n",
    "\n",
    "print(f\"\\nDATA LEAKAGE PREVENTION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Columns to drop (would not be available at loan application time):\")\n",
    "\n",
    "available_leakage_cols = [col for col in leakage_columns if col in df.columns]\n",
    "for col in available_leakage_cols:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "missing_leakage_cols = [col for col in leakage_columns if col not in df.columns]\n",
    "if missing_leakage_cols:\n",
    "    print(f\"\\nLeakage columns not found in dataset:\")\n",
    "    for col in missing_leakage_cols:\n",
    "        print(f\"  {col}\")\n",
    "\n",
    "# Combine all columns to drop\n",
    "high_missing_col_names = high_missing_cols.index.tolist()\n",
    "columns_to_drop = list(set(available_leakage_cols + high_missing_col_names))\n",
    "\n",
    "# CRITICAL: Ensure is_default is NEVER dropped (our target variable)\n",
    "if 'is_default' in columns_to_drop:\n",
    "    columns_to_drop.remove('is_default')\n",
    "    print(f\"\\nis_default protected from being dropped (our target variable)\")\n",
    "\n",
    "print(f\"\\nSUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Original columns: {len(df.columns)}\")\n",
    "print(f\"Columns to drop: {len(columns_to_drop)}\")\n",
    "print(f\"Remaining columns: {len(df.columns) - len(columns_to_drop)}\")\n",
    "print(f\"is_default protected: {'is_default' in df.columns and 'is_default' not in columns_to_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e539f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING COLUMNS\n",
      "==================================================\n",
      "Dropped 63 columns:\n",
      "  ✓ annual_inc_joint\n",
      "  ✓ application_type\n",
      "  ✓ collection_recovery_fee\n",
      "  ✓ debt_settlement_flag\n",
      "  ✓ debt_settlement_flag_date\n",
      "  ✓ deferral_term\n",
      "  ✓ desc\n",
      "  ✓ disbursement_method\n",
      "  ✓ dti_joint\n",
      "  ✓ hardship_amount\n",
      "  ✓ hardship_dpd\n",
      "  ✓ hardship_end_date\n",
      "  ✓ hardship_flag\n",
      "  ✓ hardship_last_payment_amount\n",
      "  ✓ hardship_length\n",
      "  ✓ hardship_loan_status\n",
      "  ✓ hardship_payoff_balance_amount\n",
      "  ✓ hardship_reason\n",
      "  ✓ hardship_start_date\n",
      "  ✓ hardship_status\n",
      "  ✓ hardship_type\n",
      "  ✓ id\n",
      "  ✓ il_util\n",
      "  ✓ last_credit_pull_d\n",
      "  ✓ last_pymnt_amnt\n",
      "  ✓ last_pymnt_d\n",
      "  ✓ loan_status\n",
      "  ✓ member_id\n",
      "  ✓ mths_since_last_delinq\n",
      "  ✓ mths_since_last_major_derog\n",
      "  ✓ mths_since_last_record\n",
      "  ✓ mths_since_rcnt_il\n",
      "  ✓ mths_since_recent_bc_dlq\n",
      "  ✓ mths_since_recent_revol_delinq\n",
      "  ✓ next_pymnt_d\n",
      "  ✓ orig_projected_additional_accrued_interest\n",
      "  ✓ payment_plan_start_date\n",
      "  ✓ policy_code\n",
      "  ✓ recoveries\n",
      "  ✓ revol_bal_joint\n",
      "  ✓ sec_app_chargeoff_within_12_mths\n",
      "  ✓ sec_app_collections_12_mths_ex_med\n",
      "  ✓ sec_app_earliest_cr_line\n",
      "  ✓ sec_app_inq_last_6mths\n",
      "  ✓ sec_app_mort_acc\n",
      "  ✓ sec_app_mths_since_last_major_derog\n",
      "  ✓ sec_app_num_rev_accts\n",
      "  ✓ sec_app_open_acc\n",
      "  ✓ sec_app_open_act_il\n",
      "  ✓ sec_app_revol_util\n",
      "  ✓ settlement_amount\n",
      "  ✓ settlement_date\n",
      "  ✓ settlement_percentage\n",
      "  ✓ settlement_status\n",
      "  ✓ settlement_term\n",
      "  ✓ total_pymnt\n",
      "  ✓ total_pymnt_inv\n",
      "  ✓ total_rec_int\n",
      "  ✓ total_rec_late_fee\n",
      "  ✓ total_rec_prncp\n",
      "  ✓ url\n",
      "  ✓ verification_status_joint\n",
      "  ✓ zip_code\n",
      "\n",
      "Dataset shape change:\n",
      "  Before: 2,260,668 rows × 146 columns\n",
      "  After:  2,260,668 rows × 83 columns\n",
      "  Columns reduced by: 63\n",
      "Dropped 63 columns:\n",
      "  ✓ annual_inc_joint\n",
      "  ✓ application_type\n",
      "  ✓ collection_recovery_fee\n",
      "  ✓ debt_settlement_flag\n",
      "  ✓ debt_settlement_flag_date\n",
      "  ✓ deferral_term\n",
      "  ✓ desc\n",
      "  ✓ disbursement_method\n",
      "  ✓ dti_joint\n",
      "  ✓ hardship_amount\n",
      "  ✓ hardship_dpd\n",
      "  ✓ hardship_end_date\n",
      "  ✓ hardship_flag\n",
      "  ✓ hardship_last_payment_amount\n",
      "  ✓ hardship_length\n",
      "  ✓ hardship_loan_status\n",
      "  ✓ hardship_payoff_balance_amount\n",
      "  ✓ hardship_reason\n",
      "  ✓ hardship_start_date\n",
      "  ✓ hardship_status\n",
      "  ✓ hardship_type\n",
      "  ✓ id\n",
      "  ✓ il_util\n",
      "  ✓ last_credit_pull_d\n",
      "  ✓ last_pymnt_amnt\n",
      "  ✓ last_pymnt_d\n",
      "  ✓ loan_status\n",
      "  ✓ member_id\n",
      "  ✓ mths_since_last_delinq\n",
      "  ✓ mths_since_last_major_derog\n",
      "  ✓ mths_since_last_record\n",
      "  ✓ mths_since_rcnt_il\n",
      "  ✓ mths_since_recent_bc_dlq\n",
      "  ✓ mths_since_recent_revol_delinq\n",
      "  ✓ next_pymnt_d\n",
      "  ✓ orig_projected_additional_accrued_interest\n",
      "  ✓ payment_plan_start_date\n",
      "  ✓ policy_code\n",
      "  ✓ recoveries\n",
      "  ✓ revol_bal_joint\n",
      "  ✓ sec_app_chargeoff_within_12_mths\n",
      "  ✓ sec_app_collections_12_mths_ex_med\n",
      "  ✓ sec_app_earliest_cr_line\n",
      "  ✓ sec_app_inq_last_6mths\n",
      "  ✓ sec_app_mort_acc\n",
      "  ✓ sec_app_mths_since_last_major_derog\n",
      "  ✓ sec_app_num_rev_accts\n",
      "  ✓ sec_app_open_acc\n",
      "  ✓ sec_app_open_act_il\n",
      "  ✓ sec_app_revol_util\n",
      "  ✓ settlement_amount\n",
      "  ✓ settlement_date\n",
      "  ✓ settlement_percentage\n",
      "  ✓ settlement_status\n",
      "  ✓ settlement_term\n",
      "  ✓ total_pymnt\n",
      "  ✓ total_pymnt_inv\n",
      "  ✓ total_rec_int\n",
      "  ✓ total_rec_late_fee\n",
      "  ✓ total_rec_prncp\n",
      "  ✓ url\n",
      "  ✓ verification_status_joint\n",
      "  ✓ zip_code\n",
      "\n",
      "Dataset shape change:\n",
      "  Before: 2,260,668 rows × 146 columns\n",
      "  After:  2,260,668 rows × 83 columns\n",
      "  Columns reduced by: 63\n",
      "\n",
      "✅ Feature selection complete. Working with 83 columns.\n",
      "\n",
      "✅ Feature selection complete. Working with 83 columns.\n"
     ]
    }
   ],
   "source": [
    "# Drop the identified columns\n",
    "print(\"DROPPING COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "initial_shape = df.shape\n",
    "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "final_shape = df_cleaned.shape\n",
    "\n",
    "print(f\"Dropped {len(columns_to_drop)} columns:\")\n",
    "for col in sorted(columns_to_drop):\n",
    "    if col in df.columns:\n",
    "        print(f\"  ✓ {col}\")\n",
    "\n",
    "print(f\"\\nDataset shape change:\")\n",
    "print(f\"  Before: {initial_shape[0]:,} rows × {initial_shape[1]} columns\")\n",
    "print(f\"  After:  {final_shape[0]:,} rows × {final_shape[1]} columns\")\n",
    "print(f\"  Columns reduced by: {initial_shape[1] - final_shape[1]}\")\n",
    "\n",
    "# Update our working dataframe\n",
    "df = df_cleaned.copy()\n",
    "print(f\"\\n✅ Feature selection complete. Working with {len(df.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3cdce",
   "metadata": {},
   "source": [
    "### Why Data Leakage Prevention is Critical\n",
    "\n",
    "**Data Leakage** occurs when information that would not be available at prediction time is used during model training. This creates artificially high model performance that doesn't translate to real-world scenarios.\n",
    "\n",
    "**Examples of Leakage in Loan Default Prediction**:\n",
    "- **`total_pymnt`**: Total payments made - only known after loan completion\n",
    "- **`last_pymnt_d`**: Last payment date - reveals loan performance over time  \n",
    "- **`recoveries`**: Amount recovered post-default - only known after default occurs\n",
    "- **`collection_recovery_fee`**: Fees from collections - indicates default already happened\n",
    "\n",
    "**Impact**: Models trained with leakage can achieve 99%+ accuracy in testing but fail completely in production because the \"future\" information isn't available when making real predictions.\n",
    "\n",
    "**Our Approach**: Only use information available at loan origination time, ensuring our model is realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e81a8",
   "metadata": {},
   "source": [
    "## 5. Data Type Transformations\n",
    "\n",
    "Convert string-based features to appropriate numeric formats for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abb0f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTEREST RATE TRANSFORMATION\n",
      "==================================================\n",
      "Current int_rate format (first 10 values):\n",
      "[13.56, 18.94, 17.97, 18.94, 16.14, 15.02, 17.97, 13.56, 17.97, 14.47]\n",
      "\n",
      "✅ Converted int_rate from percentage string to float\n",
      "New int_rate format (first 10 values):\n",
      "[13.56, 18.94, 17.97, 18.94, 16.14, 15.02, 17.97, 13.56, 17.97, 14.47]\n",
      "\n",
      "Interest Rate Statistics:\n",
      "  Mean: 13.09%\n",
      "  Median: 12.62%\n",
      "  Range: 5.31% - 30.99%\n",
      "\n",
      "✅ Converted int_rate from percentage string to float\n",
      "New int_rate format (first 10 values):\n",
      "[13.56, 18.94, 17.97, 18.94, 16.14, 15.02, 17.97, 13.56, 17.97, 14.47]\n",
      "\n",
      "Interest Rate Statistics:\n",
      "  Mean: 13.09%\n",
      "  Median: 12.62%\n",
      "  Range: 5.31% - 30.99%\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Convert Interest Rate from percentage string to float\n",
    "print(\"INTEREST RATE TRANSFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'int_rate' in df.columns:\n",
    "    # Check current format\n",
    "    print(\"Current int_rate format (first 10 values):\")\n",
    "    print(df['int_rate'].head(10).tolist())\n",
    "    \n",
    "    # Convert percentage string to float\n",
    "    # Remove '%' and convert to float\n",
    "    df['int_rate'] = df['int_rate'].astype(str).str.replace('%', '').astype(float)\n",
    "    \n",
    "    print(f\"\\n✅ Converted int_rate from percentage string to float\")\n",
    "    print(\"New int_rate format (first 10 values):\")\n",
    "    print(df['int_rate'].head(10).tolist())\n",
    "    \n",
    "    print(f\"\\nInterest Rate Statistics:\")\n",
    "    print(f\"  Mean: {df['int_rate'].mean():.2f}%\")\n",
    "    print(f\"  Median: {df['int_rate'].median():.2f}%\")\n",
    "    print(f\"  Range: {df['int_rate'].min():.2f}% - {df['int_rate'].max():.2f}%\")\n",
    "else:\n",
    "    print(\"⚠️ int_rate column not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b14191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TERM TRANSFORMATION\n",
      "==================================================\n",
      "Current term format (unique values):\n",
      "term\n",
      "36 months    1609754\n",
      "60 months     650914\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Converted term from string to integer\n",
      "New term format (unique values):\n",
      "term\n",
      "36    1609754\n",
      "60     650914\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Term Statistics:\n",
      "  Mean: 42.9 months\n",
      "  Most common: 36 months\n",
      "\n",
      "✅ Converted term from string to integer\n",
      "New term format (unique values):\n",
      "term\n",
      "36    1609754\n",
      "60     650914\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Term Statistics:\n",
      "  Mean: 42.9 months\n",
      "  Most common: 36 months\n"
     ]
    }
   ],
   "source": [
    "# 5.2 Convert Term from string to integer\n",
    "print(\"\\nTERM TRANSFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'term' in df.columns:\n",
    "    # Check current format\n",
    "    print(\"Current term format (unique values):\")\n",
    "    print(df['term'].value_counts())\n",
    "    \n",
    "    # Extract numeric value from term string (e.g., \" 36 months\" -> 36)\n",
    "    df['term'] = df['term'].str.extract('(\\d+)').astype(int)\n",
    "    \n",
    "    print(f\"\\n✅ Converted term from string to integer\")\n",
    "    print(\"New term format (unique values):\")\n",
    "    print(df['term'].value_counts())\n",
    "    \n",
    "    print(f\"\\nTerm Statistics:\")\n",
    "    print(f\"  Mean: {df['term'].mean():.1f} months\")\n",
    "    print(f\"  Most common: {df['term'].mode()[0]} months\")\n",
    "else:\n",
    "    print(\"⚠️ term column not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e100737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EMPLOYMENT LENGTH TRANSFORMATION\n",
      "==================================================\n",
      "Current emp_length format (value counts):\n",
      "emp_length\n",
      "10+ years    748005\n",
      "2 years      203677\n",
      "< 1 year     189988\n",
      "3 years      180753\n",
      "1 year       148403\n",
      "NaN          146907\n",
      "5 years      139698\n",
      "4 years      136605\n",
      "6 years      102628\n",
      "7 years       92695\n",
      "8 years       91914\n",
      "9 years       79395\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Converted emp_length to numeric emp_length_years\n",
      "New emp_length_years distribution:\n",
      "emp_length_years\n",
      "0.0     189988\n",
      "1.0     148403\n",
      "2.0     203677\n",
      "3.0     180753\n",
      "4.0     136605\n",
      "5.0     139698\n",
      "6.0     102628\n",
      "7.0      92695\n",
      "8.0      91914\n",
      "9.0      79395\n",
      "10.0    748005\n",
      "NaN     146907\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Employment Length Statistics:\n",
      "  Mean: 5.9 years\n",
      "  Median: 6.0 years\n",
      "  Missing values: 146,907\n",
      "\n",
      "✅ Converted emp_length to numeric emp_length_years\n",
      "New emp_length_years distribution:\n",
      "emp_length_years\n",
      "0.0     189988\n",
      "1.0     148403\n",
      "2.0     203677\n",
      "3.0     180753\n",
      "4.0     136605\n",
      "5.0     139698\n",
      "6.0     102628\n",
      "7.0      92695\n",
      "8.0      91914\n",
      "9.0      79395\n",
      "10.0    748005\n",
      "NaN     146907\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Employment Length Statistics:\n",
      "  Mean: 5.9 years\n",
      "  Median: 6.0 years\n",
      "  Missing values: 146,907\n",
      "  Dropped original emp_length column\n",
      "  Dropped original emp_length column\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Convert Employment Length to ordinal numeric feature\n",
    "print(\"\\nEMPLOYMENT LENGTH TRANSFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'emp_length' in df.columns:\n",
    "    # Check current format\n",
    "    print(\"Current emp_length format (value counts):\")\n",
    "    emp_length_counts = df['emp_length'].value_counts(dropna=False)\n",
    "    print(emp_length_counts)\n",
    "    \n",
    "    def convert_emp_length(emp_str):\n",
    "        \"\"\"Convert employment length string to numeric years\"\"\"\n",
    "        if pd.isna(emp_str):\n",
    "            return np.nan\n",
    "        \n",
    "        emp_str = str(emp_str).strip().lower()\n",
    "        \n",
    "        if '< 1 year' in emp_str or 'less than 1' in emp_str:\n",
    "            return 0\n",
    "        elif '10+ years' in emp_str:\n",
    "            return 10\n",
    "        else:\n",
    "            # Extract number from strings like \"2 years\", \"5 years\", etc.\n",
    "            match = re.search(r'(\\d+)', emp_str)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "            else:\n",
    "                return np.nan\n",
    "    \n",
    "    # Apply the conversion\n",
    "    df['emp_length_years'] = df['emp_length'].apply(convert_emp_length)\n",
    "    \n",
    "    print(f\"\\n✅ Converted emp_length to numeric emp_length_years\")\n",
    "    print(\"New emp_length_years distribution:\")\n",
    "    print(df['emp_length_years'].value_counts(dropna=False).sort_index())\n",
    "    \n",
    "    print(f\"\\nEmployment Length Statistics:\")\n",
    "    print(f\"  Mean: {df['emp_length_years'].mean():.1f} years\")\n",
    "    print(f\"  Median: {df['emp_length_years'].median():.1f} years\")\n",
    "    print(f\"  Missing values: {df['emp_length_years'].isnull().sum():,}\")\n",
    "    \n",
    "    # Drop the original emp_length column\n",
    "    df = df.drop('emp_length', axis=1)\n",
    "    print(f\"  Dropped original emp_length column\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ emp_length column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583acf9",
   "metadata": {},
   "source": [
    "## 6. Date Feature Engineering\n",
    "\n",
    "Create meaningful features from date columns to capture temporal patterns and credit history length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255b93d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE FEATURE ENGINEERING\n",
      "==================================================\n",
      "Available date columns: ['earliest_cr_line', 'issue_d']\n",
      "\n",
      "Sample date formats:\n",
      "  earliest_cr_line: ['Apr-2001', 'Jun-1987', 'Apr-2011']\n",
      "  issue_d: ['Dec-2018', 'Dec-2018', 'Dec-2018']\n",
      "\n",
      "✅ Successfully converted date columns to datetime\n",
      "\n",
      "Credit History Statistics:\n",
      "  Mean: 16.4 years\n",
      "  Median: 14.8 years\n",
      "  Range: 0.5 - 83.3 years\n",
      "  Missing values: 29\n",
      "\n",
      "✅ Successfully converted date columns to datetime\n",
      "\n",
      "Credit History Statistics:\n",
      "  Mean: 16.4 years\n",
      "  Median: 14.8 years\n",
      "  Range: 0.5 - 83.3 years\n",
      "  Missing values: 29\n",
      "\n",
      "Dropped original date columns (retained engineered features)\n",
      "\n",
      "Dropped original date columns (retained engineered features)\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Convert date columns and create credit history length\n",
    "print(\"DATE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check which date columns we have\n",
    "date_columns = ['earliest_cr_line', 'issue_d']\n",
    "available_date_cols = [col for col in date_columns if col in df.columns]\n",
    "print(f\"Available date columns: {available_date_cols}\")\n",
    "\n",
    "if 'earliest_cr_line' in df.columns and 'issue_d' in df.columns:\n",
    "    print(f\"\\nSample date formats:\")\n",
    "    print(f\"  earliest_cr_line: {df['earliest_cr_line'].dropna().iloc[0:3].tolist()}\")\n",
    "    print(f\"  issue_d: {df['issue_d'].dropna().iloc[0:3].tolist()}\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    try:\n",
    "        df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y', errors='coerce')\n",
    "        df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y', errors='coerce')\n",
    "        \n",
    "        print(f\"\\n✅ Successfully converted date columns to datetime\")\n",
    "        \n",
    "        # Create credit history length in years\n",
    "        df['credit_history_years'] = (df['issue_d'] - df['earliest_cr_line']).dt.days / 365.25\n",
    "        \n",
    "        print(f\"\\nCredit History Statistics:\")\n",
    "        print(f\"  Mean: {df['credit_history_years'].mean():.1f} years\")\n",
    "        print(f\"  Median: {df['credit_history_years'].median():.1f} years\")\n",
    "        print(f\"  Range: {df['credit_history_years'].min():.1f} - {df['credit_history_years'].max():.1f} years\")\n",
    "        print(f\"  Missing values: {df['credit_history_years'].isnull().sum():,}\")\n",
    "        \n",
    "        # Clean up - drop original date columns after feature extraction\n",
    "        df = df.drop(['earliest_cr_line', 'issue_d'], axis=1)\n",
    "        print(f\"\\nDropped original date columns (retained engineered features)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error converting dates: {e}\")\n",
    "        print(\"Skipping date feature engineering\")\n",
    "        \n",
    "elif 'earliest_cr_line' in df.columns:\n",
    "    print(\"\\n⚠️ Only earliest_cr_line available - cannot create credit history length\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Required date columns not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370f7a7",
   "metadata": {},
   "source": [
    "## 7. Handling Remaining Missing Values\n",
    "\n",
    "Apply robust imputation strategies for the remaining missing values in our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ac8eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMAINING MISSING VALUES ANALYSIS\n",
      "==================================================\n",
      "Columns with missing values (64 total):\n",
      "  all_util: 866,348 (38.3%)\n",
      "  open_acc_6m: 866,130 (38.3%)\n",
      "  inq_last_12m: 866,130 (38.3%)\n",
      "  total_cu_tl: 866,130 (38.3%)\n",
      "  inq_fi: 866,129 (38.3%)\n",
      "  open_il_12m: 866,129 (38.3%)\n",
      "  open_act_il: 866,129 (38.3%)\n",
      "  open_rv_12m: 866,129 (38.3%)\n",
      "  open_rv_24m: 866,129 (38.3%)\n",
      "  max_bal_bc: 866,129 (38.3%)\n",
      "  open_il_24m: 866,129 (38.3%)\n",
      "  total_bal_il: 866,129 (38.3%)\n",
      "  mths_since_recent_inq: 295,435 (13.1%)\n",
      "  emp_title: 166,969 (7.4%)\n",
      "  num_tl_120dpd_2m: 153,657 (6.8%)\n",
      "  emp_length_years: 146,907 (6.5%)\n",
      "  mo_sin_old_il_acct: 139,071 (6.2%)\n",
      "  bc_util: 76,071 (3.4%)\n",
      "  percent_bc_gt_75: 75,379 (3.3%)\n",
      "  bc_open_to_buy: 74,935 (3.3%)\n",
      "  mths_since_recent_bc: 73,412 (3.2%)\n",
      "  pct_tl_nvr_dlq: 70,431 (3.1%)\n",
      "  avg_cur_bal: 70,346 (3.1%)\n",
      "  num_rev_accts: 70,277 (3.1%)\n",
      "  mo_sin_old_rev_tl_op: 70,277 (3.1%)\n",
      "  mo_sin_rcnt_rev_tl_op: 70,277 (3.1%)\n",
      "  num_tl_90g_dpd_24m: 70,276 (3.1%)\n",
      "  num_tl_30dpd: 70,276 (3.1%)\n",
      "  num_rev_tl_bal_gt_0: 70,276 (3.1%)\n",
      "  num_op_rev_tl: 70,276 (3.1%)\n",
      "  tot_coll_amt: 70,276 (3.1%)\n",
      "  num_tl_op_past_12m: 70,276 (3.1%)\n",
      "  num_bc_tl: 70,276 (3.1%)\n",
      "  num_actv_rev_tl: 70,276 (3.1%)\n",
      "  num_actv_bc_tl: 70,276 (3.1%)\n",
      "  num_accts_ever_120_pd: 70,276 (3.1%)\n",
      "  num_il_tl: 70,276 (3.1%)\n",
      "  tot_cur_bal: 70,276 (3.1%)\n",
      "  mo_sin_rcnt_tl: 70,276 (3.1%)\n",
      "  tot_hi_cred_lim: 70,276 (3.1%)\n",
      "  total_rev_hi_lim: 70,276 (3.1%)\n",
      "  total_il_high_credit_limit: 70,276 (3.1%)\n",
      "  num_bc_sats: 58,590 (2.6%)\n",
      "  num_sats: 58,590 (2.6%)\n",
      "  acc_open_past_24mths: 50,030 (2.2%)\n",
      "  total_bal_ex_mort: 50,030 (2.2%)\n",
      "  total_bc_limit: 50,030 (2.2%)\n",
      "  mort_acc: 50,030 (2.2%)\n",
      "  title: 23,326 (1.0%)\n",
      "  revol_util: 1,802 (0.1%)\n",
      "  dti: 1,711 (0.1%)\n",
      "  pub_rec_bankruptcies: 1,365 (0.1%)\n",
      "  chargeoff_within_12_mths: 145 (0.0%)\n",
      "  collections_12_mths_ex_med: 145 (0.0%)\n",
      "  tax_liens: 105 (0.0%)\n",
      "  inq_last_6mths: 30 (0.0%)\n",
      "  delinq_amnt: 29 (0.0%)\n",
      "  acc_now_delinq: 29 (0.0%)\n",
      "  total_acc: 29 (0.0%)\n",
      "  pub_rec: 29 (0.0%)\n",
      "  open_acc: 29 (0.0%)\n",
      "  delinq_2yrs: 29 (0.0%)\n",
      "  credit_history_years: 29 (0.0%)\n",
      "  annual_inc: 4 (0.0%)\n",
      "\n",
      "Missing Value Categories:\n",
      "  Numeric columns: ['all_util', 'open_acc_6m', 'inq_last_12m', 'total_cu_tl', 'inq_fi', 'open_il_12m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'open_il_24m', 'total_bal_il', 'mths_since_recent_inq', 'num_tl_120dpd_2m', 'emp_length_years', 'mo_sin_old_il_acct', 'bc_util', 'percent_bc_gt_75', 'bc_open_to_buy', 'mths_since_recent_bc', 'pct_tl_nvr_dlq', 'avg_cur_bal', 'num_rev_accts', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'num_tl_90g_dpd_24m', 'num_tl_30dpd', 'num_rev_tl_bal_gt_0', 'num_op_rev_tl', 'tot_coll_amt', 'num_tl_op_past_12m', 'num_bc_tl', 'num_actv_rev_tl', 'num_actv_bc_tl', 'num_accts_ever_120_pd', 'num_il_tl', 'tot_cur_bal', 'mo_sin_rcnt_tl', 'tot_hi_cred_lim', 'total_rev_hi_lim', 'total_il_high_credit_limit', 'num_bc_sats', 'num_sats', 'acc_open_past_24mths', 'total_bal_ex_mort', 'total_bc_limit', 'mort_acc', 'revol_util', 'dti', 'pub_rec_bankruptcies', 'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'tax_liens', 'inq_last_6mths', 'delinq_amnt', 'acc_now_delinq', 'total_acc', 'pub_rec', 'open_acc', 'delinq_2yrs', 'credit_history_years', 'annual_inc']\n",
      "  Categorical columns: ['emp_title', 'title']\n",
      "Columns with missing values (64 total):\n",
      "  all_util: 866,348 (38.3%)\n",
      "  open_acc_6m: 866,130 (38.3%)\n",
      "  inq_last_12m: 866,130 (38.3%)\n",
      "  total_cu_tl: 866,130 (38.3%)\n",
      "  inq_fi: 866,129 (38.3%)\n",
      "  open_il_12m: 866,129 (38.3%)\n",
      "  open_act_il: 866,129 (38.3%)\n",
      "  open_rv_12m: 866,129 (38.3%)\n",
      "  open_rv_24m: 866,129 (38.3%)\n",
      "  max_bal_bc: 866,129 (38.3%)\n",
      "  open_il_24m: 866,129 (38.3%)\n",
      "  total_bal_il: 866,129 (38.3%)\n",
      "  mths_since_recent_inq: 295,435 (13.1%)\n",
      "  emp_title: 166,969 (7.4%)\n",
      "  num_tl_120dpd_2m: 153,657 (6.8%)\n",
      "  emp_length_years: 146,907 (6.5%)\n",
      "  mo_sin_old_il_acct: 139,071 (6.2%)\n",
      "  bc_util: 76,071 (3.4%)\n",
      "  percent_bc_gt_75: 75,379 (3.3%)\n",
      "  bc_open_to_buy: 74,935 (3.3%)\n",
      "  mths_since_recent_bc: 73,412 (3.2%)\n",
      "  pct_tl_nvr_dlq: 70,431 (3.1%)\n",
      "  avg_cur_bal: 70,346 (3.1%)\n",
      "  num_rev_accts: 70,277 (3.1%)\n",
      "  mo_sin_old_rev_tl_op: 70,277 (3.1%)\n",
      "  mo_sin_rcnt_rev_tl_op: 70,277 (3.1%)\n",
      "  num_tl_90g_dpd_24m: 70,276 (3.1%)\n",
      "  num_tl_30dpd: 70,276 (3.1%)\n",
      "  num_rev_tl_bal_gt_0: 70,276 (3.1%)\n",
      "  num_op_rev_tl: 70,276 (3.1%)\n",
      "  tot_coll_amt: 70,276 (3.1%)\n",
      "  num_tl_op_past_12m: 70,276 (3.1%)\n",
      "  num_bc_tl: 70,276 (3.1%)\n",
      "  num_actv_rev_tl: 70,276 (3.1%)\n",
      "  num_actv_bc_tl: 70,276 (3.1%)\n",
      "  num_accts_ever_120_pd: 70,276 (3.1%)\n",
      "  num_il_tl: 70,276 (3.1%)\n",
      "  tot_cur_bal: 70,276 (3.1%)\n",
      "  mo_sin_rcnt_tl: 70,276 (3.1%)\n",
      "  tot_hi_cred_lim: 70,276 (3.1%)\n",
      "  total_rev_hi_lim: 70,276 (3.1%)\n",
      "  total_il_high_credit_limit: 70,276 (3.1%)\n",
      "  num_bc_sats: 58,590 (2.6%)\n",
      "  num_sats: 58,590 (2.6%)\n",
      "  acc_open_past_24mths: 50,030 (2.2%)\n",
      "  total_bal_ex_mort: 50,030 (2.2%)\n",
      "  total_bc_limit: 50,030 (2.2%)\n",
      "  mort_acc: 50,030 (2.2%)\n",
      "  title: 23,326 (1.0%)\n",
      "  revol_util: 1,802 (0.1%)\n",
      "  dti: 1,711 (0.1%)\n",
      "  pub_rec_bankruptcies: 1,365 (0.1%)\n",
      "  chargeoff_within_12_mths: 145 (0.0%)\n",
      "  collections_12_mths_ex_med: 145 (0.0%)\n",
      "  tax_liens: 105 (0.0%)\n",
      "  inq_last_6mths: 30 (0.0%)\n",
      "  delinq_amnt: 29 (0.0%)\n",
      "  acc_now_delinq: 29 (0.0%)\n",
      "  total_acc: 29 (0.0%)\n",
      "  pub_rec: 29 (0.0%)\n",
      "  open_acc: 29 (0.0%)\n",
      "  delinq_2yrs: 29 (0.0%)\n",
      "  credit_history_years: 29 (0.0%)\n",
      "  annual_inc: 4 (0.0%)\n",
      "\n",
      "Missing Value Categories:\n",
      "  Numeric columns: ['all_util', 'open_acc_6m', 'inq_last_12m', 'total_cu_tl', 'inq_fi', 'open_il_12m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'open_il_24m', 'total_bal_il', 'mths_since_recent_inq', 'num_tl_120dpd_2m', 'emp_length_years', 'mo_sin_old_il_acct', 'bc_util', 'percent_bc_gt_75', 'bc_open_to_buy', 'mths_since_recent_bc', 'pct_tl_nvr_dlq', 'avg_cur_bal', 'num_rev_accts', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'num_tl_90g_dpd_24m', 'num_tl_30dpd', 'num_rev_tl_bal_gt_0', 'num_op_rev_tl', 'tot_coll_amt', 'num_tl_op_past_12m', 'num_bc_tl', 'num_actv_rev_tl', 'num_actv_bc_tl', 'num_accts_ever_120_pd', 'num_il_tl', 'tot_cur_bal', 'mo_sin_rcnt_tl', 'tot_hi_cred_lim', 'total_rev_hi_lim', 'total_il_high_credit_limit', 'num_bc_sats', 'num_sats', 'acc_open_past_24mths', 'total_bal_ex_mort', 'total_bc_limit', 'mort_acc', 'revol_util', 'dti', 'pub_rec_bankruptcies', 'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'tax_liens', 'inq_last_6mths', 'delinq_amnt', 'acc_now_delinq', 'total_acc', 'pub_rec', 'open_acc', 'delinq_2yrs', 'credit_history_years', 'annual_inc']\n",
      "  Categorical columns: ['emp_title', 'title']\n"
     ]
    }
   ],
   "source": [
    "# Analyze remaining missing values after feature engineering\n",
    "print(\"REMAINING MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_after_engineering = df.isnull().sum()\n",
    "missing_cols = missing_after_engineering[missing_after_engineering > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Columns with missing values ({len(missing_cols)} total):\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {col}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Separate numeric and categorical columns for different imputation strategies\n",
    "    numeric_cols_with_missing = []\n",
    "    categorical_cols_with_missing = []\n",
    "    \n",
    "    for col in missing_cols.index:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            numeric_cols_with_missing.append(col)\n",
    "        else:\n",
    "            categorical_cols_with_missing.append(col)\n",
    "    \n",
    "    print(f\"\\nMissing Value Categories:\")\n",
    "    print(f\"  Numeric columns: {numeric_cols_with_missing}\")\n",
    "    print(f\"  Categorical columns: {categorical_cols_with_missing}\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ No missing values remaining in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df62c341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "APPLYING IMPUTATION STRATEGIES\n",
      "==================================================\n",
      "Numeric columns - using MEDIAN imputation:\n",
      "  ✓ all_util: filled 866,348 missing values with 58.00\n",
      "  ✓ open_acc_6m: filled 866,130 missing values with 1.00\n",
      "  ✓ inq_last_12m: filled 866,130 missing values with 1.00\n",
      "  ✓ total_cu_tl: filled 866,130 missing values with 0.00\n",
      "  ✓ inq_fi: filled 866,129 missing values with 1.00\n",
      "  ✓ open_il_12m: filled 866,129 missing values with 0.00\n",
      "  ✓ open_act_il: filled 866,129 missing values with 2.00\n",
      "  ✓ open_rv_12m: filled 866,129 missing values with 1.00  ✓ open_rv_12m: filled 866,129 missing values with 1.00\n",
      "  ✓ open_rv_24m: filled 866,129 missing values with 2.00\n",
      "  ✓ max_bal_bc: filled 866,129 missing values with 4413.00\n",
      "  ✓ open_il_24m: filled 866,129 missing values with 1.00\n",
      "  ✓ total_bal_il: filled 866,129 missing values with 23127.00\n",
      "  ✓ mths_since_recent_inq: filled 295,435 missing values with 5.00\n",
      "  ✓ num_tl_120dpd_2m: filled 153,657 missing values with 0.00\n",
      "  ✓ emp_length_years: filled 146,907 missing values with 6.00\n",
      "\n",
      "  ✓ open_rv_24m: filled 866,129 missing values with 2.00\n",
      "  ✓ max_bal_bc: filled 866,129 missing values with 4413.00\n",
      "  ✓ open_il_24m: filled 866,129 missing values with 1.00\n",
      "  ✓ total_bal_il: filled 866,129 missing values with 23127.00\n",
      "  ✓ mths_since_recent_inq: filled 295,435 missing values with 5.00\n",
      "  ✓ num_tl_120dpd_2m: filled 153,657 missing values with 0.00\n",
      "  ✓ emp_length_years: filled 146,907 missing values with 6.00\n",
      "  ✓ mo_sin_old_il_acct: filled 139,071 missing values with 130.00\n",
      "  ✓ bc_util: filled 76,071 missing values with 60.20\n",
      "  ✓ percent_bc_gt_75: filled 75,379 missing values with 37.50\n",
      "  ✓ bc_open_to_buy: filled 74,935 missing values with 5442.00\n",
      "  ✓ mths_since_recent_bc: filled 73,412 missing values with 14.00\n",
      "  ✓ pct_tl_nvr_dlq: filled 70,431 missing values with 100.00\n",
      "  ✓ mo_sin_old_il_acct: filled 139,071 missing values with 130.00\n",
      "  ✓ bc_util: filled 76,071 missing values with 60.20\n",
      "  ✓ percent_bc_gt_75: filled 75,379 missing values with 37.50\n",
      "  ✓ bc_open_to_buy: filled 74,935 missing values with 5442.00\n",
      "  ✓ mths_since_recent_bc: filled 73,412 missing values with 14.00\n",
      "  ✓ pct_tl_nvr_dlq: filled 70,431 missing values with 100.00\n",
      "  ✓ avg_cur_bal: filled 70,346 missing values with 7335.00\n",
      "  ✓ num_rev_accts: filled 70,277 missing values with 12.00\n",
      "  ✓ mo_sin_old_rev_tl_op: filled 70,277 missing values with 164.00\n",
      "  ✓ mo_sin_rcnt_rev_tl_op: filled 70,277 missing values with 8.00\n",
      "  ✓ num_tl_90g_dpd_24m: filled 70,276 missing values with 0.00\n",
      "  ✓ num_tl_30dpd: filled 70,276 missing values with 0.00\n",
      "  ✓ num_rev_tl_bal_gt_0: filled 70,276 missing values with 5.00\n",
      "  ✓ avg_cur_bal: filled 70,346 missing values with 7335.00\n",
      "  ✓ num_rev_accts: filled 70,277 missing values with 12.00\n",
      "  ✓ mo_sin_old_rev_tl_op: filled 70,277 missing values with 164.00\n",
      "  ✓ mo_sin_rcnt_rev_tl_op: filled 70,277 missing values with 8.00\n",
      "  ✓ num_tl_90g_dpd_24m: filled 70,276 missing values with 0.00\n",
      "  ✓ num_tl_30dpd: filled 70,276 missing values with 0.00\n",
      "  ✓ num_rev_tl_bal_gt_0: filled 70,276 missing values with 5.00\n",
      "  ✓ num_op_rev_tl: filled 70,276 missing values with 7.00\n",
      "  ✓ tot_coll_amt: filled 70,276 missing values with 0.00\n",
      "  ✓ num_tl_op_past_12m: filled 70,276 missing values with 2.00\n",
      "  ✓ num_bc_tl: filled 70,276 missing values with 7.00\n",
      "  ✓ num_actv_rev_tl: filled 70,276 missing values with 5.00\n",
      "  ✓ num_actv_bc_tl: filled 70,276 missing values with 3.00\n",
      "  ✓ num_accts_ever_120_pd: filled 70,276 missing values with 0.00\n",
      "  ✓ num_op_rev_tl: filled 70,276 missing values with 7.00\n",
      "  ✓ tot_coll_amt: filled 70,276 missing values with 0.00\n",
      "  ✓ num_tl_op_past_12m: filled 70,276 missing values with 2.00\n",
      "  ✓ num_bc_tl: filled 70,276 missing values with 7.00\n",
      "  ✓ num_actv_rev_tl: filled 70,276 missing values with 5.00\n",
      "  ✓ num_actv_bc_tl: filled 70,276 missing values with 3.00\n",
      "  ✓ num_accts_ever_120_pd: filled 70,276 missing values with 0.00\n",
      "  ✓ num_il_tl: filled 70,276 missing values with 6.00\n",
      "  ✓ tot_cur_bal: filled 70,276 missing values with 79240.00\n",
      "  ✓ mo_sin_rcnt_tl: filled 70,276 missing values with 6.00\n",
      "  ✓ tot_hi_cred_lim: filled 70,276 missing values with 114298.50\n",
      "  ✓ total_rev_hi_lim: filled 70,276 missing values with 25400.00\n",
      "  ✓ total_il_high_credit_limit: filled 70,276 missing values with 32696.00\n",
      "  ✓ num_il_tl: filled 70,276 missing values with 6.00\n",
      "  ✓ tot_cur_bal: filled 70,276 missing values with 79240.00\n",
      "  ✓ mo_sin_rcnt_tl: filled 70,276 missing values with 6.00\n",
      "  ✓ tot_hi_cred_lim: filled 70,276 missing values with 114298.50\n",
      "  ✓ total_rev_hi_lim: filled 70,276 missing values with 25400.00\n",
      "  ✓ total_il_high_credit_limit: filled 70,276 missing values with 32696.00\n",
      "  ✓ num_bc_sats: filled 58,590 missing values with 4.00\n",
      "  ✓ num_sats: filled 58,590 missing values with 11.00\n",
      "  ✓ acc_open_past_24mths: filled 50,030 missing values with 4.00\n",
      "  ✓ total_bal_ex_mort: filled 50,030 missing values with 37864.00\n",
      "  ✓ total_bc_limit: filled 50,030 missing values with 16300.00\n",
      "  ✓ mort_acc: filled 50,030 missing values with 1.00\n",
      "  ✓ num_bc_sats: filled 58,590 missing values with 4.00\n",
      "  ✓ num_sats: filled 58,590 missing values with 11.00\n",
      "  ✓ acc_open_past_24mths: filled 50,030 missing values with 4.00\n",
      "  ✓ total_bal_ex_mort: filled 50,030 missing values with 37864.00\n",
      "  ✓ total_bc_limit: filled 50,030 missing values with 16300.00\n",
      "  ✓ mort_acc: filled 50,030 missing values with 1.00\n",
      "  ✓ revol_util: filled 1,802 missing values with 50.30\n",
      "  ✓ dti: filled 1,711 missing values with 17.84\n",
      "  ✓ pub_rec_bankruptcies: filled 1,365 missing values with 0.00\n",
      "  ✓ chargeoff_within_12_mths: filled 145 missing values with 0.00\n",
      "  ✓ collections_12_mths_ex_med: filled 145 missing values with 0.00\n",
      "  ✓ tax_liens: filled 105 missing values with 0.00\n",
      "  ✓ inq_last_6mths: filled 30 missing values with 0.00\n",
      "  ✓ delinq_amnt: filled 29 missing values with 0.00\n",
      "  ✓ acc_now_delinq: filled 29 missing values with 0.00\n",
      "  ✓ revol_util: filled 1,802 missing values with 50.30\n",
      "  ✓ dti: filled 1,711 missing values with 17.84\n",
      "  ✓ pub_rec_bankruptcies: filled 1,365 missing values with 0.00\n",
      "  ✓ chargeoff_within_12_mths: filled 145 missing values with 0.00\n",
      "  ✓ collections_12_mths_ex_med: filled 145 missing values with 0.00\n",
      "  ✓ tax_liens: filled 105 missing values with 0.00\n",
      "  ✓ inq_last_6mths: filled 30 missing values with 0.00\n",
      "  ✓ delinq_amnt: filled 29 missing values with 0.00\n",
      "  ✓ acc_now_delinq: filled 29 missing values with 0.00\n",
      "  ✓ total_acc: filled 29 missing values with 22.00\n",
      "  ✓ pub_rec: filled 29 missing values with 0.00\n",
      "  ✓ open_acc: filled 29 missing values with 11.00\n",
      "  ✓ delinq_2yrs: filled 29 missing values with 0.00\n",
      "  ✓ credit_history_years: filled 29 missing values with 14.83\n",
      "  ✓ annual_inc: filled 4 missing values with 65000.00\n",
      "\n",
      "Categorical columns - using MODE imputation:\n",
      "  ✓ total_acc: filled 29 missing values with 22.00\n",
      "  ✓ pub_rec: filled 29 missing values with 0.00\n",
      "  ✓ open_acc: filled 29 missing values with 11.00\n",
      "  ✓ delinq_2yrs: filled 29 missing values with 0.00\n",
      "  ✓ credit_history_years: filled 29 missing values with 14.83\n",
      "  ✓ annual_inc: filled 4 missing values with 65000.00\n",
      "\n",
      "Categorical columns - using MODE imputation:\n",
      "  ✓ emp_title: filled 166,969 missing values with 'Teacher'\n",
      "  ✓ title: filled 23,326 missing values with 'Debt consolidation'\n",
      "  ✓ emp_title: filled 166,969 missing values with 'Teacher'\n",
      "  ✓ title: filled 23,326 missing values with 'Debt consolidation'\n",
      "\n",
      "✅ Imputation complete. Total missing values: 0\n",
      "\n",
      "Why MEDIAN over MEAN for numeric features?\n",
      "  • Median is robust to outliers and skewed distributions\n",
      "  • Mean can be heavily influenced by extreme values\n",
      "  • Financial data often has outliers (high earners, large loans)\n",
      "  • Median preserves the central tendency better\n",
      "\n",
      "✅ Imputation complete. Total missing values: 0\n",
      "\n",
      "Why MEDIAN over MEAN for numeric features?\n",
      "  • Median is robust to outliers and skewed distributions\n",
      "  • Mean can be heavily influenced by extreme values\n",
      "  • Financial data often has outliers (high earners, large loans)\n",
      "  • Median preserves the central tendency better\n"
     ]
    }
   ],
   "source": [
    "# Apply imputation strategies\n",
    "print(\"\\nAPPLYING IMPUTATION STRATEGIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    # For numeric columns: use median (robust to outliers)\n",
    "    if numeric_cols_with_missing:\n",
    "        print(\"Numeric columns - using MEDIAN imputation:\")\n",
    "        for col in numeric_cols_with_missing:\n",
    "            original_missing = df[col].isnull().sum()\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].fillna(median_value)\n",
    "            print(f\"  ✓ {col}: filled {original_missing:,} missing values with {median_value:.2f}\")\n",
    "    \n",
    "    # For categorical columns: use mode (most frequent value)\n",
    "    if categorical_cols_with_missing:\n",
    "        print(f\"\\nCategorical columns - using MODE imputation:\")\n",
    "        for col in categorical_cols_with_missing:\n",
    "            original_missing = df[col].isnull().sum()\n",
    "            mode_value = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "            df[col] = df[col].fillna(mode_value)\n",
    "            print(f\"  ✓ {col}: filled {original_missing:,} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    final_missing = df.isnull().sum().sum()\n",
    "    print(f\"\\n✅ Imputation complete. Total missing values: {final_missing}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No imputation needed - dataset already complete!\")\n",
    "\n",
    "print(f\"\\nWhy MEDIAN over MEAN for numeric features?\")\n",
    "print(f\"  • Median is robust to outliers and skewed distributions\")\n",
    "print(f\"  • Mean can be heavily influenced by extreme values\")\n",
    "print(f\"  • Financial data often has outliers (high earners, large loans)\")\n",
    "print(f\"  • Median preserves the central tendency better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17670d70",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data\n",
    "\n",
    "Save our cleaned and engineered dataset in an optimized format for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9b3db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATASET SUMMARY\n",
      "==================================================\n",
      "Final dataset shape: 2,260,668 rows × 82 columns\n",
      "Memory usage: 2626.7 MB\n",
      "Missing values: 0\n",
      "✅ is_default column confirmed in final dataset\n",
      "   Default rate: 0.126\n",
      "   Class distribution: {0: 1976324, 1: 284344}\n",
      "\n",
      "Final features (82 total):\n",
      "Columns in final dataset: ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'home_ownership', 'annual_inc', 'verification_status', 'pymnt_plan', 'purpose', 'title', 'addr_state', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'total_bal_il', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'is_default', 'emp_length_years', 'credit_history_years']\n",
      "Data type distribution:\n",
      "  float64: 67 columns\n",
      "  object: 10 columns\n",
      "  int64: 5 columns\n",
      "\n",
      "Saving processed dataset to: ../data/processed/processed_loan_data.parquet\n",
      "Missing values: 0\n",
      "✅ is_default column confirmed in final dataset\n",
      "   Default rate: 0.126\n",
      "   Class distribution: {0: 1976324, 1: 284344}\n",
      "\n",
      "Final features (82 total):\n",
      "Columns in final dataset: ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title', 'home_ownership', 'annual_inc', 'verification_status', 'pymnt_plan', 'purpose', 'title', 'addr_state', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'out_prncp', 'out_prncp_inv', 'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'total_bal_il', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'is_default', 'emp_length_years', 'credit_history_years']\n",
      "Data type distribution:\n",
      "  float64: 67 columns\n",
      "  object: 10 columns\n",
      "  int64: 5 columns\n",
      "\n",
      "Saving processed dataset to: ../data/processed/processed_loan_data.parquet\n",
      "✅ Dataset saved successfully!\n",
      "   File size: 200.0 MB\n",
      "✅ Dataset saved successfully!\n",
      "   File size: 200.0 MB\n",
      "   Compression ratio: 13.1x\n",
      "   Compression ratio: 13.1x\n",
      "✅ Verified: is_default exists in saved file\n",
      "✅ Verified: is_default exists in saved file\n"
     ]
    }
   ],
   "source": [
    "# Final data summary before saving\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# CRITICAL VERIFICATION: Ensure is_default exists in final dataset\n",
    "if 'is_default' in df.columns:\n",
    "    print(f\"✅ is_default column confirmed in final dataset\")\n",
    "    print(f\"   Default rate: {df['is_default'].mean():.3f}\")\n",
    "    print(f\"   Class distribution: {df['is_default'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(f\"❌ ERROR: is_default column missing from final dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    raise ValueError(\"is_default target variable is missing from final dataset\")\n",
    "\n",
    "# Display final feature list\n",
    "print(f\"\\nFinal features ({len(df.columns)} total):\")\n",
    "print(f\"Columns in final dataset: {list(df.columns)}\")\n",
    "feature_types = df.dtypes.value_counts()\n",
    "print(f\"Data type distribution:\")\n",
    "for dtype, count in feature_types.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Ensure processed directory exists\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save to parquet format\n",
    "output_path = processed_dir / 'processed_loan_data.parquet'\n",
    "print(f\"\\nSaving processed dataset to: {output_path}\")\n",
    "\n",
    "df.to_parquet(output_path, index=False, compression='snappy')\n",
    "\n",
    "# Verify the saved file\n",
    "if output_path.exists():\n",
    "    file_size_mb = output_path.stat().st_size / 1024**2\n",
    "    print(f\"✅ Dataset saved successfully!\")\n",
    "    print(f\"   File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"   Compression ratio: {df.memory_usage(deep=True).sum() / 1024**2 / file_size_mb:.1f}x\")\n",
    "    \n",
    "    # Double-check by loading the saved file\n",
    "    df_verify = pd.read_parquet(output_path)\n",
    "    if 'is_default' in df_verify.columns:\n",
    "        print(f\"✅ Verified: is_default exists in saved file\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: is_default missing from saved file!\")\n",
    "else:\n",
    "    print(\"❌ Error: File not saved properly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325297ac",
   "metadata": {},
   "source": [
    "Parquet format is used because it is fast, efficient, and preserves data integrity for machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0270cbd",
   "metadata": {},
   "source": [
    "## 9. Version Processed Data with DVC\n",
    "\n",
    "**Critical MLOps Step**: Add our processed dataset to DVC tracking for reproducible ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939dff0",
   "metadata": {},
   "source": [
    "### DVC Versioning Commands\n",
    "\n",
    "Execute these commands in your terminal to version the processed dataset:\n",
    "\n",
    "```bash\n",
    "# 1. Add processed data to DVC tracking\n",
    "dvc add data/processed/processed_loan_data.parquet\n",
    "\n",
    "# 2. Add DVC pointer files to Git\n",
    "git add data/processed/processed_loan_data.parquet.dvc data/processed/.gitignore\n",
    "\n",
    "# 3. Commit the feature engineering work\n",
    "git add notebooks/02-Feature-Engineering.ipynb\n",
    "git commit -m \"feat: Phase 2 complete - Feature engineering and data preprocessing\n",
    "\n",
    "- Created comprehensive feature engineering pipeline\n",
    "- Engineered binary default target (is_default) \n",
    "- Removed data leakage by dropping post-application features\n",
    "- Converted categorical features: int_rate, term, emp_length\n",
    "- Created credit_history_years from date features\n",
    "- Applied robust median imputation for missing values\n",
    "- Saved optimized dataset in Parquet format (50%+ compression)\n",
    "- Versioned processed data with DVC for reproducible ML pipeline\n",
    "\n",
    "Dataset ready for model training: {df.shape[0]:,} rows × {df.shape[1]} features\"\n",
    "```\n",
    "\n",
    "### What This Achieves:\n",
    "- **Reproducible Pipeline**: Anyone can recreate the exact processed dataset\n",
    "- **Data Lineage**: Clear track from raw data to processed features  \n",
    "- **Version Control**: Changes to preprocessing are tracked and reversible\n",
    "- **Team Collaboration**: Shared processed datasets across team members\n",
    "- **Production Ready**: Processed data ready for model training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43bdd9",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "**Completed Steps:**\n",
    "1. Created the `is_default` target variable for default prediction.\n",
    "2. Removed columns with high missing values and those causing data leakage.\n",
    "3. Transformed features: converted interest rate and term to numeric, employment length to ordinal, and engineered credit history years.\n",
    "4. Imputed missing values using median for numeric and mode for categorical features.\n",
    "5. Saved the processed data in Parquet format for efficient storage and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
